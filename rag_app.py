import tempfile
import whisper
from pytube import YouTube

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import DocArrayInMemorySearch
from langchain_openai.embeddings import OpenAIEmbeddings

from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.prompts import ChatPromptTemplate


def yt_transcript(video_url, name_txt):
    """Create transcript of a youtube video (txt doc)

    Args:
        video_url (str): url ofthe video to transcript
        name_txt (str): name of the transcription (txt doc)
    """
    youtube = YouTube(video_url)
    audio = youtube.streams.filter(only_audio=True).first()

    whisper_model = whisper.load_model("base")

    with tempfile.TemporaryDirectory() as tmpdir:
        file = audio.download(output_path=tmpdir)
        transcription = whisper_model.transcribe(file, fp16=False)["text"].strip()

        with open(name_txt, "w") as file:
            file.write(transcription)


def transcript_to_document(transcript_name, size=1000, overlap=20):
    """The transcript needs to be split to be fed to ChatGpt (limited amount of input token)
    With this function we can split the documents into fixed-size chunks

    Args:
        transcript_name (str): name of the txt doc to be split
        size (int): number of token in per chunk
        overlap (int): number of token that overlap between 2 chunk

    Returns:
        list: list of all the chunks from the original document (langchain format)
    """

    loader = TextLoader(transcript_name)
    text_documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=size, chunk_overlap=overlap
    )
    documents = text_splitter.split_documents(text_documents)
    return documents


def create_vectore_store(documents):
    """Storage of the document chunk embeddings in a vector storage, to be able to search at scale
    vectore store = database of embeddings that specializes in fast similarity searches

    Args:
        documents (list langchain): list of chunks from the original text document

    Returns:
        vectorestore (langchain format): vectore storage for the document
    """
    embeddings = OpenAIEmbeddings()
    vectorstore = DocArrayInMemorySearch.from_documents(documents, embeddings)
    return vectorstore


def chat_bot_chain(vectorstore, model):
    """Mapping of the langchain workflow
    - retrieve the context from the vector store (embeddings of the document splitted)
    - prompt from a langchain template (chatbot style)
    - model to use
    - parser to use

    Args:
        vectorstore (store of vectors): embeddings of the document
        model (ai model) : ai model to use to get the answers

    Returns:
        chain: chained steps
    """
    parser = StrOutputParser()

    template = """
        Answer the question based on the context below. If you can't 
        answer the question, reply "I don't know".

        Context: {context}

        Question: {question}
        """

    prompt = ChatPromptTemplate.from_template(template)

    chain = (
        {"context": vectorstore.as_retriever(), "question": RunnablePassthrough()}
        | prompt
        | model
        | parser
    )

    return chain


def chat_bot(question, chain):
    """Returns the answer generated by the ai model

    Args:
        question (str): question of the user
        chain (langchain seq): chained step to go through to get the answer

    Returns:
        str: answer
    """
    return chain.invoke(question)
